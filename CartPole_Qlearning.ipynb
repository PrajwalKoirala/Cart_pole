{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python\\Python37\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')       # creates CartPole environment\n",
    "new_state = env.reset()             # env.reset() resets the cartpole and returns the new state\n",
    "done =   False                      # whether the simulation of one episode has completed on not\n",
    "actions = [0, 1]                    # list of possible actions : 0 corresponds to \"pushing the cart to the left\" and 1 correspinds to \"pushing the cart to the left\"\n",
    "\n",
    "discount = 0.97                     # discount rate\n",
    "epsilon = 1                         # probability of exploring other possibilities instead of following the policy (we will be decreasing this value with every iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random normal initialization of 2*4 matrix W: \n",
    "W = tf.get_variable(\"W\", [2,4], initializer = tf.random_normal_initializer(stddev = 0.1), dtype=tf.float64)   \n",
    "# create a placeholder for new_state, which we will be feeding every iteration: \n",
    "New_state = tf.placeholder(tf.float64, shape = (4,1))   \n",
    "# Multiply W and New_state to obtain Q vector   ( or Q(s) ) :\n",
    "Q_s = tf.matmul(W, New_state)  \n",
    "# We will be taking the action with maximum Q value, Q_sa is the Q value associated with the action\n",
    "Action = tf.math.argmax(Q_s) \n",
    "Q_sa = tf.reduce_max(Q_s)\n",
    "# We pass this action to the environment and it will return us reward and the next state\n",
    "# So, create a placeholder for reward and next state, which we will be feeding every iteration: \n",
    "Reward = tf.placeholder(tf.float64, shape = (1,1))\n",
    "Next_state = tf.placeholder(tf.float64, shape = (4,1))\n",
    "#Bellman equation: Current_Q_value =  Reward + future Q value\n",
    "# We expect our W to predict this\n",
    "Target = Reward + discount * tf.reduce_max(tf.matmul(W, Next_state))\n",
    "#squared error is our cost\n",
    "Cost = tf.square(Target - Q_sa)\n",
    "train = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start tensorlow session and initialize global variables\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# in case you want to save the best working W\n",
    "best_reward = -np.Inf\n",
    "best_W1 = sess.run(W)\n",
    "# number of episodes of simulation\n",
    "Episodes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 499\n",
      "epsilon: 1.7216336019101414e-105\n",
      "total rewards of last episode: 23.0\n",
      "best reward: 498.0\n",
      "W: \n",
      "[[ 0.27603316  0.08331281  0.31661391  0.00605107]\n",
      " [-0.136452   -0.05279967  0.03332765 -0.00640894]]\n"
     ]
    }
   ],
   "source": [
    "for episode in range(Episodes):\n",
    "    total_reward = 0           # to keep track of total_reward in this episode\n",
    "    done = False               # initialize every episode with done = False\n",
    "    new_state = env.reset()    # reset the environment every episode\n",
    "    if episode%49==0:          # show/render environment only after every 49 episodes, to speed up the training  (yeah i am odd)\n",
    "        show = True\n",
    "    else:\n",
    "        show = False\n",
    "    while not done:\n",
    "        # feed the placeholder New_state with new_state to get Action\n",
    "        action  = sess.run(Action, feed_dict = {New_state:new_state.reshape((4,1))})      \n",
    "        state = new_state         \n",
    "        # two possible actions : random_action with probability of epsilon, and q_action with probabilty of 1-epsilon\n",
    "        random_action = np.random.choice(actions)\n",
    "        q_action = actions[action[0]]\n",
    "        action_that_gets_passed = np.array([random_action if np.random.rand() < epsilon else q_action])\n",
    "        # pass action_that_gets_passed to update the state and get some reward!\n",
    "        new_state, reward, done, _ = env.step(action_that_gets_passed[0])\n",
    "        reward = reward if not done else -reward   # because this env gives positive reward even when simulation terminates\n",
    "        total_reward += reward              # total _reward in this episode\n",
    "        # feed values to respective placeholders and compute train\n",
    "        feed = {Action:action_that_gets_passed, Reward: np.array(reward).reshape(1,1), New_state:state.reshape((4,1)), Next_state:new_state.reshape((4,1))}\n",
    "        sess.run([train, Cost], feed_dict = feed) \n",
    "        epsilon = epsilon*0.99       # decrease epsilon, decrease randomness!\n",
    "        if show:\n",
    "            env.render()\n",
    "    if total_reward>=best_reward:      # save the best W \n",
    "        best_reward = total_reward\n",
    "        best_W = sess.run(W)\n",
    "    clear_output(wait=True)                    # clear output every episode and replace with new (for jupyter notebook)\n",
    "    print(\"episode: \" + str(episode))          # print current episode            \n",
    "    print(\"epsilon: \" + str(epsilon))          # print current epsilon\n",
    "    print(\"total rewards of last episode: \" + str(total_reward))           # total rewards  of  last epsode\n",
    "    print(\"best reward: \" + str(best_reward))                              # best reward till now\n",
    "    print(\"W: \")                                                           # enjoy watching while the optimizer tweaks your W matrix\n",
    "    print(sess.run(W))\n",
    "            \n",
    "env.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n"
     ]
    }
   ],
   "source": [
    "assign =  W.assign(best_W)\n",
    "sess.run(assign)\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "new_state = env.reset()\n",
    "while not done:\n",
    "        action  = sess.run(Action, feed_dict = {New_state:new_state.reshape((4,1))})\n",
    "        new_state, reward, done, _ = env.step(action[0])\n",
    "        total_reward += reward\n",
    "        env.render()\n",
    "        \n",
    "print(total_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
